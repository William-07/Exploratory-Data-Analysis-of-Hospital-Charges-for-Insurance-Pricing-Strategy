---
output:
  pdf_document: default
  html_document: default
---
---
title: "DSCC/CSC/STAT 462 Assignment 3"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: Due November 2, 2023 by 11:59 p.m.
fontsize: 12pt
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, eval=T,tidy=TRUE, tidy.opts=list(width.cutoff=60))
```
  
\vspace{5pt}
Please complete this assignment using `RMarkdown`, and submit the knitted PDF. *For all hypothesis tests, state the hypotheses, report the test statistic and p-value, and comment on the results in the context of the problem.* 
\vspace{5pt}

In order to run hypothesis tests and construct confidence intervals, you may find the `z.test` and/or `t.test` functions in `R` to be useful. For documentation, run `?z.test` and/or `?t.test` in the console.

1. Recently there has been much concern regarding fatal police shootings, particularly in relation to a victim's race (with "victim" being used generally to describe the person who was fatally shot). Since the start of 2015, the Washington Post has been collecting data on every fatal shooting in America by a police office who was on duty. A subset of that data is presented in the dataset "shootings.csv."
    a. A recent census study indicates that the average age of Americans is 40 years old. Conduct a hypothesis test "by-hand" (i.e., do not use the `t.test()` function, but still use `R`) at the $\alpha=0.05$ significance level to see if the average age of victims is significantly different from 40 years old.
    ```{r, echo=T}
    shootings_data <- read.csv("shootings.csv")
    #shootings_data
    ```  
    Hypothesis:  
    $H_{0} : \mu_{0} = 40$  
    $H_{1} : \mu_{0} \neq  40$  
    ```{r, echo=T}
    #define parameter
    u_0 <- 40
    x_bar <- mean(shootings_data$age)
    varinace <- var(shootings_data$age)
    n  <- length(shootings_data$age)
    df = n-1
    #t-value
    t_value <- (x_bar - u_0)/sqrt(varinace/n)
    print(paste('t-value= ',t_value))
    p_value = 2*(1-pt(t_value,df))
    print(paste('p-value= ',p_value))
    
    ```  
    The p_value is 0.106849585109261 > 0.05, so we fail to rejecct $H_{0}$.  
    The average age of victims is not significantly different from 40 years old.  
    
    b. At the $\alpha=0.01$ significance level, test "by-hand" (i.e., do not use the `t.test()` function, but still use `R`) whether the average age of minority victims is different than the average age of non-minority victims. Assume equal variances. 
    
    ```{r, echo=T}
    #seperate two gorups
     Minority <- subset(shootings_data, minority == 'yes')
     Non_minority <- subset(shootings_data, minority == 'no')
    ```  
    Hypothesis:  
      $H_{0}:\mu_{non-minority} = \mu_{minority}$  
      $H_{1}:\mu_{non-minority} \neq \mu_{minority}$
      
    ```{r, echo=T}
      n_nm <- length(Non_minority$age)
      n_m <- length(Minority$age)
      x_bar_nm <- mean(Non_minority$age)
      x_bar_m <- mean(Minority$age)
      Variance_nm <- var(Non_minority$age)
      Variance_m <- var(Minority$age)
      Variance_pool <- ((n_nm -1)*Variance_nm+(n_m - 1)*Variance_m)/((n_nm -1)+(n_m - 1))
      df = n_nm-1+n_m-1
      #t-test
      t_value <- (x_bar_nm - x_bar_m)/sqrt(Variance_pool*(1/n_nm+1/n_m))
      print(paste('t-value= ',t_value))
      p_value <- 2*(1-pt(t_value,df))
      print(paste('p-value= ',p_value))
      
    ```  
    p-value = 0.00440256049101251 < 0.01  
    reject $H_{0}$, the average age of minority victims is different than the average age of non-minority victims.  
2. In the dataset named "blackfriday.csv," there is information relating to the amount of money that a sample of $n=31$ consumers spent shopping on Black Friday in 2017. 
    ```{r, echo=T}
    blackfriday_data <- read.csv("blackfriday.csv")
    #blackfriday_data
    ```  
    a. A company is interested in determining an upper-bound on the mean amount of money spent on Black Friday in order to determine maximum effects on the economy. Construct a one-sided upper-bound 99% confidence interval "by-hand" (i.e., do not use the `t.test()` function, but still use `R`) for the mean amount of money spent on Black Friday. Interpret the results. 

    ```{r, echo=T}  
    #define parameter
    x_bar <- mean(blackfriday_data$Amount)
    standard_deviation <- sd(blackfriday_data$Amount)
    n  <- 31
    #confidence interval upper bound
    U = x_bar+qt(1-0.01,31-1)*standard_deviation/sqrt(n)
    print(U)
    ```  
    The upper bound of the 99% confidence interval of the mean amount of money spent on Black Friday is 13577.85. It means that there is 99% of probability that the mean of money spent on Black Friday will fall in the interval from $-\infty$ to 13577.85
    b. Suppose that in 2018, the average amount spent shopping on Black Friday was \$12000. Based on your sample, is there evidence to conclude that the mean amount spent shopping on Black Friday is 2017 is less than \$12000? Conduct an appropriate hypothesis test "by-hand" (i.e., do not use the `t.test()` function, but still use `R`) at the $\alpha=0.05$ significance level.  
    Hypothesis:  
    $H_{0} : \mu_{0} \geq 12000$  
    $H_{1} : \mu_{0} <  12000$  
    ```{r, echo=T} 
    u_0 <- 12000
    x_bar <- mean(blackfriday_data$Amount)
    varinace <- var(blackfriday_data$Amount)
    n  <- length(blackfriday_data$Amount)
    df = n-1
    #t-value
    t_value <- (x_bar - u_0)/sqrt(varinace/n)
    print(paste('t-value= ',t_value))
    p_value = (pt(t_value,df))
    print(paste('p-value= ',p_value))
    ```  
    The p-value is 0.200394944181881 > 0.05.We fail to reject the $H_{0}$. The evidence is not enough to conclude that the mean amount spent shopping on Black Friday in 2017 is less than 12000

3. The Duke Chronicle collected data on all 1739 students listed in the Class of 2018's "Freshmen Picture Book." In particular, the Duke Chronicle examined hometowns, details about the students' high schools, whether they won a merit scholarship, and their sports team involvement. Ultimately, the goal was to determine trends between those who do and do not join Greek life at the university. A subset of this data is contained in the file named "greek.csv." The variable `greek` is an indicator that equals 1 if the student is involved in Greek life and 0 otherwise. The variable `hstuition` gives the amount of money spent on the student's high school tuition. 
    a. At the $\alpha=0.1$ significance level, test whether the average high school tuition for a student who does not partake in Greek life is less than the average high school tuition for a student who does partake in Greek life. Assume unequal variances.
    ```{r, echo=T}
    greek_data <- read.csv("greek.csv")
    No_Greek = subset(greek_data,greek == 0)
    Greek = subset(greek_data,greek == 1)
    ```  
    Hypothesis:  
    $H_{0}:\mu_{NG} \geq \mu_{G}$  
    $H_{1}:\mu_{NG} < \mu_{G}$
    ```{r, echo=T}
    t.test(x = No_Greek$hstuition, y =Greek$hstuition,alt = "less", var.equal=F, conf.level=0.9)
    ```  
    The p-value is 0.00441 < 0.1. We reject the $H_{0}$. The average high school tuition for a student who does not partake in Greek life is less than the average high school tuition for a student who does partake in Greek life. 
    
    b. Construct a one-sided, lower-bound 90% confidence interval on the mean amount of high school tuition paid by Duke students. Interpret the result. 
    ```{r, echo=T}  
    #define parameter
    x_bar <- mean(greek_data$hstuition)
    standard_deviation <- sd(greek_data$hstuition)
    n  <- length(greek_data$hstuition)
    #confidence interval upper bound
    L = x_bar-qt(0.9,n-1)*standard_deviation/sqrt(n)
    print(L)  
    ```  
    The lower-bound 90% confidence interval on the mean amount of high school tuition paid by Duke students is 25386.16. It means there are 90 percent of the probability that the mean is higher than 25386.16.
  
4. Seven trumpet players are given a new breathing exercise to help with their breath support. The trumpet players are asked to play a C note for as long as they can both before and after the breathing exercise. The time (in seconds) that they can hold the note for are presented below. Assume times are normally distributed. 
\begin{center}
\begin{tabular}{cccccccc}
  \hline
 Subject & 1	&2&	3&	4&	5	&6	&7\\
 \hline
Before & 9.1	&11.2	&11.9	&14.7	&11.7	&9.5	&14.2		\\
After & 10.7&	14.2	&12.4	&14.6	&16.4	&10.1	&19.2	\\
   \hline
\end{tabular}
\end{center}

Perform an appropriate test at the $\alpha=0.1$ significance level to determine if the mean time holding a note is greater after the exercise than before.  
    $d = \mu_{beofre} - \mu_{after}$  
    $H_{0}:d\leq0$  
    $H_{1}:d>0$  
```{r, echo=T} 
    #pair data
    #load data
    breath_before <- c(9.1, 11.2, 11.9, 14.7, 11.7, 9.5, 14.2)
    breath_after <- c(10.7, 14.2, 12.4, 14.6, 16.4, 10.1, 19.2)
    difference <- breath_before - breath_after
    t.test(x = difference, alt = 'less',conf.level=0.9)
```   
The p-value is 0.01585 < 0.1. We reject $H_{0}$. The mean time is greater after the exercise than before.  

5. Let $\mu$ be the average amount of time in minutes spent on social media apps each day. Based on an earlier study, it is hypothesized that $\mu=124$ minutes. It is believed, though, that people are spending increasingly more time on social media apps during the pandemic. We sample $n$ people and determine the average amount of time spent on social media apps per day in order to test the hypotheses $H_0: \mu \le 124$ vs. $H_1:\mu>124$, at the $\alpha=0.01$ significance level. Suppose we know that $\sigma=26$ minutes. 
    a. Create a sequence of reasonable alternative values for $\mu$. Take $\mu_1 \in (124,190)$, using `seq(124,190, by=0.001)` in `R`.
    ```{r, echo=T} 
    mu1 <- seq(124,190, by=0.001)
    ```  
    b. Use `R` to draw a power curve for when $n=5$. You may find the `plot()` function useful. In particular, `plot(mu1, __, type = "l", ylab = "Power", xlab = expression(mu[1]))` could be a useful starting point for formatting.
    
    ```{r, echo=T} 
    #define parameter
    mu0 <- 124
    n <- 5
    sd <- 26
    power <- numeric(length(mu1))
    z_reject <- qnorm(0.99)
    x_bar_reject <- z_reject*sd/sqrt(n)+mu0
    #calculate the power
    for (i in seq_along(mu1)){
      Z <- (x_bar_reject - mu1[i])/(sd/sqrt(n))
      #print(Z)
      power[i] <- 1-pnorm(Z)
    }
    plot(mu1, power, type = "l", ylab = "Power", xlab = expression(mu[1]))
    ```  
    c. Using the same general plot as part b, draw power curves for when the sample size equals $n=5,15,25,50$. You can do this using the `lines()` function in place of when you used `plot()` in part b. Make the curve for each of these a different color, and add a legend to distinguish these curves. 
    ```{r, echo=T} 
    #define parameter
    mu0 <- 124
    n <- c(5,15,25,50)
    sd <- 26
    power <- list(numeric(length(mu1)),numeric(length(mu1)),numeric(length(mu1)),numeric(length(mu1)))
    z_reject <- qnorm(0.99)
    #calculate the power
    for (j in seq_along(n)){
      for (i in seq_along(mu1)){
        x_bar_reject <- z_reject*sd/sqrt(n[j])+mu0
        Z <- (x_bar_reject - mu1[i])/(sd/sqrt(n[j]))
        power[[j]][i] <- 1-pnorm(Z)
      }
    }
    plot(mu1, power[[1]], type = "l", ylab = "Power", xlab = expression(mu[1]))
    lines(mu1, power[[2]], type="l", col="red")
    lines(mu1, power[[3]], type="l", col="green")
    lines(mu1, power[[4]], type="l", col="blue")
    
    # Add a legend
    legend("bottomright",
    legend = c(paste("n =", n[1]), paste("n =", n[2]), paste("n =", n[3]), paste("n =", n[4])),  # Legend text
       col = c("black", "red", "green", "blue"),lty = 1)
    ```  
    d. What is the power of this test when $\mu_1=141$ and $n=28$?
    ```{r, echo=T} 
    #define parameter
    mu0 <- 124
    n <- 28
    sd <- 26
    mu1 <- 141
    z_reject <- qnorm(0.99)
    x_bar_reject <- z_reject*sd/sqrt(n)+mu0
    #calculate the power
    Z <- (x_bar_reject - mu1)/(sd/sqrt(n))
    power <- 1-pnorm(Z)
    print(paste("The power is ", power))
    ```  
    e. How large of a sample size is needed to attain a power of $0.95$ when the true mean amount of time on social media apps is $\mu_1=128$?
    ```{r, echo=T} 
    power <- 0.95
    mu1 <- 128
    mu0 <- 124
    sd <- 26
    z_a <- qnorm(0.99)
    z_b <- qnorm(0.95)
    n <- ceiling((sd*(z_a+z_b)/(mu1-mu0))^2)
    print(paste("The sample size should be: ",n))
    
    ``` 
6. Data collected on $n=83$ Air BnB listings in New York City are contained in the file "airbnb.csv." Read this file into R and create two new variables, one for the price of full house rentals and one for the price of private room rentals.
    a. At the $\alpha=0.05$ level, test "by-hand" (i.e., do not use any `.test()` function, but still use `R`) whether the variance of price of entire home rentals is significantly different from the variance of price of private home rentals. 
    ```{r, echo=T}
    #Read this file into R and create two new variables,
     airbnb_data <- read.csv("airbnb.csv")
     Full_house <- subset(airbnb_data,room_type == "Entire home")
     private_room <- subset(airbnb_data,room_type == "Private room")
     Full_house_price <- Full_house$price
     private_room_price <- private_room$price
    ``` 
    Hypotheses:  
      $H_{0}:\sigma_{full}^2 = \sigma_{private}^2$  
      $H_{1}:\sigma_{full}^2 \neq \sigma_{private}^2$
    ```{r, echo=T} 
    #define parameter
    f <- var(Full_house_price)/var(private_room_price)
    p_value <- 2*(1-pf(f,length(Full_house_price)-1,length(private_room_price)-1))
    print(paste("The p_value is :",p_value))
    ``` 
    The p_value is 0 < 0.05. We reject $H_{0}$. The variance of price of entire home rentals is significantly different from the variance of price of private home rentals.  
    b. At the $\alpha=0.05$ level, test "by-hand" (i.e., do not use any `.test()` function, but still use `R`) whether the variance of price of private room rentals is significantly different from $40^2$.  
    Hypotheses:  
      $H_{0}: \sigma_{private}^2 = 40^2$  
      $H_{1}:\sigma_{private}^2 \neq 40^2$
    ```{r, echo=T} 
    #define parameter
    t <- (length(private_room_price)-1)*var(private_room_price)/(40^2)
    p_value <- 2*(1-pchisq(t,length(private_room_price)-1))
    print(paste("The p_value is :",p_value))
    ``` 
    The p_value is 0.0102508316992849 < 0.05. We reject $H_{0}$. The variance of price of private room rentals is significantly different from $40^2$.  
7. A gaming store is interested in exploring the gaming trends of teenagers. A random sample of $143$ teenagers is taken. From this sample, the gaming store observes that 95 teenagers play videos games regularly. For all parts of this problem, do the calculation "by-hand" (i.e., do not use the `prop.test()` or `binom.test()` functions, but still use `R`).
    a. Construct a two-sided 95% (Wald) confidence interval for the proportion of all teenagers who play video games regularly. 
    ```{r, echo=T}
      n <- 143
      p_head <- 95/n
      z <- qnorm(1-0.025)
      lower_bound <- p_head - z*sqrt(p_head*(1-p_head)/n)
      upper_bound <- p_head + z*sqrt(p_head*(1-p_head)/n)
      print(paste("The 95% confidence two-sided interval is(",lower_bound,upper_bound,")"))
    ```  
    b. A teen magazine advertises that "74% of teenagers play video game regularly," and you want to see if this claim is true. Perform a hypothesis test at the $\alpha=0.05$ significance level to test whether this claim is correct.  
    Hypothesis:  
      $H_{0}:p = 0.74$  
      $H_{1}: p \neq 0.74$
    
    ```{r, echo=T}
    n <- 143
    p_head  <- 95/143
    p_0 <- 0.74
    z <- (p_head-p_0)/sqrt((p_0*(1-p_0))/n)
    p_value <- 2*pnorm(z)
    print(paste("The p_value is: ", p_value))
    ```  
    The p-value is 0.0391318212000937 < 0.05. We reject $H_{0}$. The claim is not correct.
    c. Comment on how comparable the results are from the confidence interval and the hypothesis test in examining the teen magazine's claim. Explain.  
      We will get different results if we apply the confidence interval for our test. We approach the confidence interval by letting the true proportion be equal to the sample proportion. However, we do the hypothesis test by letting the true proportion equal our hypothesis proportion which is 0.74. In this case, the confidence interval shows that 0.74 is in the interval. It could be a true proportion but the hypothesis test shows that we should reject 0.74 as our true proportion, which is contradictory.  
      
8. Researchers at a Las Vegas casino want to determine what proportion of its visitors smoke while in the casino. Casino executives are planning to conduct a survey, and they are willing to have a margin of error of 0.07 in estimating the true proportion of visitors who  smoke. If the executives want to create a two-sided 99\% (Wald) confidence interval, how many visitors must be included in the study?
    ```{r, echo=T}
    m = 0.07
    z = qnorm(1-(0.01/2))
    p = 0.5
    n = (z^2*p*(1-p))/m^2
    print(paste("The sample size should be ",ceiling(n)))
    ``` 
9. Are people in Australia more likely to have pets than people in America? Of a sample of 51 Australians, 32 indicated having a pet. In an independent sample of 63 Americans, 27 indicated having a pet. Test "by-hand" (i.e., do not use the `prop.test()` or `binom.test()` functions, but still use `R`) at the $\alpha=0.05$ significance level whether the proportion of Australians who have pets is greater than the proportion of Americans who have pets.  
    Hypothesis:  
      $H_{0}: p_us \geq p_as$  
      $H_{1}: p_as < p_us$
    ```{r, echo=T}
      p_us_head = 27/63
      p_as_head = 32/51
      p_head = (27+32)/(63+51)
      z =  (p_us_head -  p_as_head)/sqrt(p_head*(1-p_head)*(1/63+1/51))
      p_value = pnorm(z)
      print(paste("The p-value is ",p_value))
      
    ```  
    The p-value is 0.0173022426565621<0.05. We reject $H_{0}$. The proportion of Australians who have pets is greater than the proportion of Americans who have pets
    
Short Answers:

  * About how long did this assignment take you? Did you feel it was too long, too short, or reasonable? 
    8hours
  * Who, if anyone, did you work with on this assignment?
    Only myself
  * What questions do you have relating to any of the material we have covered so far in class?  
    So far so good